\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx,psfrag,epsf}
\usepackage{enumerate}
\usepackage{natbib}

\newcommand{\blind}{0}

\addtolength{\oddsidemargin}{-.75in}%
\addtolength{\evensidemargin}{-.75in}%
\addtolength{\textwidth}{1.5in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%


\begin{document}


%\bibliographystyle{natbib}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\if0\blind
{
  \title{\bf Applications of Supervised Learning to Parkinson's Disease Discrimination from Dysphonia Measurements}
  \author{Michael Walton\\
    Department of Computer Science, Georgia Institute of Technology\\}
  \maketitle
} \fi

\if1\blind
{
  \bigskip
  \bigskip
  \bigskip
  \begin{center}
    {\LARGE\bf Title}
\end{center}
  \medskip
} \fi

\bigskip
\begin{abstract}
In this work we explore the application of a variety of supervised Machine Learning algorithms to a Parkinson's Disease (PD) speech pathology dataset. Our objective is the development of models capable of discriminating unseen Parkinsonian voice samples from normative (PD-negative) controls. Predictive classification models are principally assessed by comparing their performance as a function of training set sample size. Following from the methods employed in the associated study which accompanies this dataset, we also explore the impact of representing individual subjects' data using central tendency and dispersion measures across multiple samples. Ultimately, we have been able to derive models with comparable PD-discriminating performance to the originating study.
\end{abstract}

\spacingset{1.45}
\section{Introduction}
\label{sec:intro}

Parkinson's disease (PD) is a neurodegeneritive central nervous system disorder characterized by progressive loss in motor control, speech and cognitive function. Common symptoms include tremor, difficulty speaking (dysphonia) and dementia. Most treatments for PD are effective only in the early motor stages of the disease which makes early detection crucial for improving patients' quality of life and long-term prognosis.

Today, the majority of PD cases are diagnosed using the Unified Parkinson's Disease Rating Scale (UPDRS). This behavioral test panel assigns a numeric value which encodes the progression of the disease. The observation tests must be administered by a qualified neurologist making PD detection a time consumptive and costly procedure. The desire for a rapid, low-cost pre-screening diagnostic has motivated a number of studies which attempt to distinguish early and difficult to detect PD symptoms such subtle aperiodic fluctuations in voice recordings. Although a thorough description of these metrics is beyond the scope of this study, it suffices to understand that clinical speech pathology literature has identified vocal acoustic irregularities such as turbulent non-Gaussian noise in early stage PD patients. Determining if variability in these phonations can yeild a robust PD diagnostic remains a subject of active research.

One such study, conducted by Sakar et al. \cite{Sakar2013} gathered audio samples from 20 persons with Parkinson's (PWP) and 20 otherwise healthy adults. The researchers gathered a series of voice samples from participants and extracted a set of 26 features using various linear and time-frequency based metrics such as jitter, shimmer and pitch. In the present study we define our instance space as the 26 dimensional space of acoustic features sampled in \cite{Sakar2013}. The target hypothesis is a consistent function mapping elements from the instance space to a binary classification label denoting Parkinson's Disease (PD+) or cognitively normative (PD-). To derive a maximum a-posteriori hypothesis given the aforementioned dataset, we split the data into distinct training and testing subsets and minimize error of a collection of learning algorithms on the training set. The generalization error on the test set is also recorded and compared.

In \cite{Sakar2013} the researchers also propose a means of reducing the impact of inter-trial variability in samples taken from the same subject. This is accomplished by taking the mean and standard deviation of samples from a common participant such that \textit{m} participant samples map to a single instance for each participant. In effect, this increases the dimensionality of the instance space by a factor of two, while reducing the number of available samples by a factor of \textit{m}; making the number of samples equivalent to the number of participants. It is of interest in this study, and the Machine Learning course generally, to understand the relationship between the dimensionality of the input and number of training examples to generalization performance of a learning algorithm. To keep things interesting, we conjecture (in opposition to the results in \cite{Sakar2013}) that performing this preprocessing step may in fact hinder some algorithms' performance due to the curse of dimensionality in conjunction with a restriction of the number of samples available for training.

The supervised learning algorithms considered in the present study are \textit{k}-Nearest Neighbor, Support Vector Machines, Artificial Neural Networks, Decision Trees and Boosting. The python modules \texttt{scipy, numpy,} and \texttt{scikit-learn} \cite{scipy} \cite{scikit-learn} are used extensively  throughout our implementation; the GPU-accelerated math compiler Theano \cite{bergstra+al:2010-scipy} is used in conjunction with convenience wrappers \texttt{lasagne} and \texttt{nolearn} for ANNs; Keka \cite{Hall2009} is utilized for pruned decision trees and boosted models.\cite{Tsanas2010}

\section{Methods}
\label{sec:meth}
In the sections that follow, we will provide a brief overview of the dysphonia datasets used in this study and the assumptions made in working with these data. In addition, we will briefly discuss the implementation details for each considered algorithm and highlight any model-specific transformations of the data or optimization procedures.

\subsection{Data}
The two datasets considered in this study originate from Parkinson's Disease clinical diagnostic literature; specifically, from a research domain interested in correlating vocal dysphonia metrics with the disease. In the first study, researchers collected voice recordings from 20 persons with Parkinson's (PWP) and 20 otherwise healthy adults. Multiple types of sound recordings were collected from each participant (including sustained vowels, words, numbers and short sentences). The raw audio samples were then processed into a set of 26 features using various linear and time-frequency based metrics such as jitter, shimmer and pitch. The input data $X \in \mathbb{R}^{n \times m} $ where \textit{n} is the number of audio samples (\textit{n} = 1040) and \textit{m} is the dimensionality of the feature space (\textit{m} = 26); target labels are contained in a binary vector $ \boldsymbol{y} \in \mathbb{R}^n $. We therefore define our instance space as the 26 dimensional space of vocal dysphonia features sampled in \cite{Sakar2013}. The target hypothesis is a data-consistent function $ h : x \in \mathbb{R}^m \mapsto y \in \lbrace +,- \rbrace $. The objective is some tbd. 'best' function which partitions the instance space by mapping each example $ x \in X $ to a corresponding binary classification label $ y \in \boldsymbol{y} $ denoting Parkinson's Disease (PD+) or cognitively normative (PD-).

The second dataset considered in our analysis contains similar dysphonia features to \cite{Sakar2013}, however instances are associated with real-valued, continuous UPDRS scores. These data may be readily modeled as a regression problem; in order to render the data suitable for classification we generate proxy labels from the original distribution of UPDRS scores which represent the continuous scale as discrete 'stages' (the number of stages may be arbitrary, 5 has been selected so as to maintain consistency with the Parkinson's literature). In the associated study, researchers gathered a total of 5,875 voice recordings from 42 PWP. The vocal dysphonia metrics used are largely overlapping with \cite{Sakar2013}, however fewer metrics are considered. We define our target hypothesis as a consistent function $ h : x \in \mathbb{R}^m \mapsto y \in [1,5] $ where \textit{x, y, m} are defined similarly to the previous study (\textit{m} = 16).

\includegraphics[scale=0.5]{updrs}

Although the two datasets are very similar in their methods of data collection and overall approach, there are several key differences worth bearing in mind. The shape of these data is interesting in that: the Sakar study possesses less samples and a higher dimensional instance space when compared to Tsanas; it is possible that insufficient data and the curse of dimensionality play a significant factor in the generalization error of algorithms applied to Sakar. However, the sample distribution over classes in Sakar is perfectly uniform (20 in both PD+ and PD- classes), as opposed to the roughly gaussian distribution of UPDRS scores in Tsanas leading to imbalanced classes after partitioning the samples.

Taking these observations further, given the low occurrence of Parkinsons, the real-world performance of a model trained on uniformally distributed samples will be far lower than the estimated generalization error during training and validation. This is due to the fact that the a-priori distribution is only artificially uniform, when in fact the true probability of a randomly selected individual having Parkinson's is only about 0.00013. Though this is still a significant portion, it is difficult to make strong assertions about clinical significance and diagnostic value in this context. In the Tsanas study, however, participants are assumed to be PD+ and the learning task is simply the prediction of the progression of the disease from voice samples. In this context, it seems much more likely that the prior distribution of the collected samples more closely match the true distribution of PWP.

\subsection{Training}
To derive maximum a-posteriori hypotheses given the aforementioned datasets, we apply the following preprocessing and training procedures which are roughly the same for all algorithms used:
\begin{itemize}
  \item Separate the raw-data feature columns \textit{X} from the label column \textbf{\textit{y}}
  \item Randomly shuffle and split the data into distinct training and testing sets (1/3 held out for testing)
  \item Scale the data to zero mean and unit variance using \texttt{StandardScaler} (the transformation is computed only from the training set, then applied to all data)
  \item Train each algorithm using stratified \textit{k}-folds cross validation ($k=5$ in Sakar experiments, $k=3$ for Tsanas)\footnote{A smaller \textit{k} was used on Tsanas data to ensure that the distribution of each fold could match as closely as possible to the complete set of training labels}
\end{itemize}

\subsection{Algorithms and Evaluation}
f1 score, loss fn for nn
\section{Results}
\subsection{Model Evaluation}
\subsection{Performance Comparison}

\section{Conclusion}
\label{sec:conc}


\bigskip
\begin{center}
{\large\bf SUPPLEMENTAL MATERIALS}
\end{center}

\begin{description}

\item[Source Code:] The algorithms discussed in the present study along with data-handling, post-hoc statistics and visualizations are implemented in the form of an ipython notebook (parkinsons.ipynb)

\item[Dependencies Install Script:] A python script which will fetch and install the  python modules required by the ipython notebook (dependencies.py)

\item[Parkinson's Dataset:] Data set used in this study is contained in the folder pd-msr-data. This folder contains both raw audio samples (.wma) and extracted dysphonia measurements (.csv)

\end{description}

\bibliographystyle{plain}
\bibliography{ml-p1.bib}

\end{document} 